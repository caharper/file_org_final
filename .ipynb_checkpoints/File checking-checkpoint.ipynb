{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on the idea that users can initialize the database with 3 files (a routes file, a driver file, and a driver assignment file).  However, once these files are loaded, the user cannot append to the data that we are storing through an additional csv file.  For example, if a user initialized the DB with ./routes_file_1, they cannot also append to the routes with a ./routes_file_2.  This is important for a number of reasons that I don't want to write out but we can talk about them if we want.  We could allow for a \"clear DB\" button and allow the user to re-initialize though.\n",
    "\n",
    "We probably want to refactor into using parquet files like here: https://stackoverflow.com/questions/61920105/dask-applying-a-function-over-a-large-dataframe-which-is-more-than-ram\n",
    "\n",
    "Not a huge change and the current code should pretty much work when we do that. Waiting to do this though because some of the operations that we need to check for depend on the whole dataframe and I'm still learning Dask. \n",
    "\n",
    "Some of the implementation could be done a lot more efficiently, but for now I left it this way explicitly so that it is clear how it operates.  We can adjust later.\n",
    "\n",
    "\n",
    "\n",
    "**Don't use dask.compute()  Puts everything in main memory!**\n",
    "\n",
    "**Use pip to install dask if you want to install yourself.  Had issues with conda**\n",
    "\n",
    "**I added a .yml file so that we can just use the same conda environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use dask or sframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import multiprocessing\n",
    "n_cpus = multiprocessing.cpu_count()\n",
    "n_cpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/clayharper/anaconda/envs/db_project_dask/lib/python3.7/site-packages/dask/dataframe/core.py:6194: UserWarning: Insufficient elements for `head`. 5 elements requested, only 1 elements available. Try passing larger `npartitions` to `head`.\n",
      "  warnings.warn(msg.format(n, len(r)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>route_name</th>\n",
       "      <th>src_city_name</th>\n",
       "      <th>src_state_code</th>\n",
       "      <th>dest_city_name</th>\n",
       "      <th>dest_state_code</th>\n",
       "      <th>route_type</th>\n",
       "      <th>dep_time_hr</th>\n",
       "      <th>dep_time_min</th>\n",
       "      <th>travel_time_hr</th>\n",
       "      <th>travel_time_min</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A1345</td>\n",
       "      <td>None</td>\n",
       "      <td>Waco</td>\n",
       "      <td>TX</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>TX</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      route_id route_name src_city_name src_state_code dest_city_name  \\\n",
       "index                                                                   \n",
       "2        A1345       None          Waco             TX         Dallas   \n",
       "\n",
       "      dest_state_code route_type dep_time_hr dep_time_min travel_time_hr  \\\n",
       "index                                                                      \n",
       "2                  TX          0          20            0             72   \n",
       "\n",
       "      travel_time_min  \n",
       "index                  \n",
       "2                   0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DaskReader():\n",
    "    ''' Base class for reading csv files with Dask.  \n",
    "\n",
    "    '''\n",
    "    def __init__(self, csv_path):\n",
    "        # Verify that the path extension is .csv\n",
    "        self._verify_csv_format(csv_path)\n",
    "        self.csv_path = csv_path\n",
    "        \n",
    "        self.state_codes = [\"AL\", \"AK\", \"AZ\", \"AR\", \"CA\", \"CO\", \"CT\",\n",
    "                            \"DC\", \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\", \n",
    "                            \"IN\", \"IA\", \"KS\", \"KY\", \"LA\", \"ME\", \"MD\", \n",
    "                            \"MA\", \"MI\", \"MN\", \"MS\", \"MO\", \"MT\", \"NE\",\n",
    "                            \"NV\", \"NH\", \"NJ\", \"NM\", \"NY\", \"NC\", \"ND\",\n",
    "                            \"OH\", \"OK\", \"OR\", \"PA\", \"RI\", \"SC\", \"SD\", \n",
    "                            \"TN\", \"TX\", \"UT\", \"VT\", \"VA\", \"WA\", \"WV\", \n",
    "                            \"WI\", \"WY\"]\n",
    "        \n",
    "    def _verify_csv_format(self, file):\n",
    "        if Path(file).suffix == '.csv':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _is_nan(self, x):\n",
    "        if x != x or x is None:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def _verify_str_len(self, x, min_len, max_len):\n",
    "        if self._is_nan(x):\n",
    "            return False\n",
    "        \n",
    "        if len(x) < min_len or len(x) > max_len:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _verify_id(self, x):\n",
    "        if self._is_nan(x):\n",
    "            return False\n",
    "        \n",
    "        if not x.isalnum() or len(x) != 5:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def _verify_int_value(self, x, min_x, max_x):\n",
    "        # make sure is an int\n",
    "        if not x.isdigit():\n",
    "            return False\n",
    "        \n",
    "        int_x = int(x)\n",
    "        if int_x < min_x or int_x > max_x:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def _is_empty(self, df):\n",
    "        if len(df.index) == 0:\n",
    "            return True\n",
    "        return False\n",
    "                \n",
    "    def _read_df(self, file_type, names=None):\n",
    "        '''Get a dask dataframe from the file\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        names : list of str names for columns in a csv file.\n",
    "            This assumes that the correct amount of names is passed into this \n",
    "            function so that it matches up with the csv file. This also assumes\n",
    "            that the csv files do not have a header with column names initially.\n",
    "            By saving as a group of parquet files, we keep the operations from\n",
    "            causing memory issues.\n",
    "        '''\n",
    "        df = dd.read_csv(self.csv_path, header=None, dtype='str', names=names)\n",
    "        df.repartition(partition_size=\"100MB\")\n",
    "        \n",
    "        # save as parquet files\n",
    "        parquet_path = './parquet_processing/' + file_type + '/'\n",
    "        df.to_parquet(parquet_path)\n",
    "#         print(dd.read_parquet(parquet_path).head())\n",
    "        return dd.read_parquet(parquet_path)\n",
    "    \n",
    "# class AssignmentReader(DaskReader):\n",
    "#     def __init__(self, csv_path):\n",
    "#         super().__init__(csv_path)\n",
    "        \n",
    "class DriverReader(DaskReader):\n",
    "    ''' Reads driver csv files.\n",
    "    \n",
    "    Expected Columns\n",
    "    ----------------\n",
    "    driver_id (unique)\n",
    "    last_name\n",
    "    first_name\n",
    "    age (years)\n",
    "    home_city\n",
    "    home_state(standard US state code, 2 characters)\n",
    "    \n",
    "    It is assumed that the company only hires drivers that are from a city\n",
    "    where a bus goes to.  No need to verify that there is a route to the \n",
    "    home city based on the assumption given in the assignment.\n",
    "    \n",
    "    \"The company only hire drivers from cities where there is a route that \n",
    "    serves as its destination (destination ONLY, not departure city).\"\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__(csv_path)\n",
    "        \n",
    "        self.column_names = ['driver_id', 'last_name', 'first_name', 'age', 'home_city',\n",
    "                            'home_state']\n",
    "        \n",
    "        self.df = self.read_df()\n",
    "        \n",
    "    def read_df(self):\n",
    "        return self._read_df(file_type='drivers', names=self.column_names)\n",
    "    \n",
    "    def verify_df(self):\n",
    "        self._verify_attributes()\n",
    "        \n",
    "    def _check_duplicates_by(self, column_name):\n",
    "        # adapted from: https://github.com/dask/dask/issues/2952\n",
    "        self.df = self.df.reset_index().drop_duplicates([column_name]).set_index(\"index\")\n",
    "    \n",
    "    def _verify_attributes(self):\n",
    "        '''Verifies basic attributes in the table. \n",
    "        \n",
    "        We verify that each of the attributes follows the datastructures we are placing on them,\n",
    "        and we may want to add functionality to drop any rows that are duplicates.  Note: we \n",
    "        must check if the dataframe is empty so that we avoid KeyErrors.\n",
    "        '''\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # driver_id must be 5 characters and alphanumeric\n",
    "        self.df = self.df[self.df['driver_id'].apply(\n",
    "            lambda x: self._verify_id(x), meta=pd.Series([], dtype='str', name='x'))]\n",
    "\n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # last_name, first_name are a max of 80 characters\n",
    "        self.df = self.df[self.df['last_name'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 80), meta=pd.Series([], dtype='str', name='x'))]\n",
    "\n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        self.df = self.df[self.df['first_name'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 80), meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # age is up to 3 characters and is an integer from 16-100\n",
    "        self.df = self.df[self.df['age'].apply(\n",
    "            lambda x: self._verify_str_len(x, 2, 3) and self._verify_int_value(x, 16, 100),\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class RouteReader(DaskReader):\n",
    "    ''' Reads route csv files.\n",
    "    \n",
    "    Expected Columns\n",
    "    ----------------\n",
    "    Route number\n",
    "    Route name (left empty if not present)\n",
    "    Departure city name\n",
    "    Departure city code (standard US state code, 2 characters)\n",
    "    Destination city name\n",
    "    Destination city code\n",
    "    Route type code (0 for daily, 1 for weekdays only, 2for weekend only)\n",
    "    Departure time (hours)\n",
    "    Departure time (minutes)\n",
    "    Travel time (hours)\n",
    "    Travel time (minutes)\n",
    "    \n",
    "    I renamed them below so it is harder to accidentially call on the wrong\n",
    "    name (i.e. departure is now src)\n",
    "    \n",
    "    Helpful link: https://stackoverflow.com/questions/47125665/simple-dask-map-partitions-example\n",
    "    '''\n",
    "    def __init__(self, csv_path):\n",
    "        super().__init__(csv_path)\n",
    "        \n",
    "        self.column_names = ['route_id', 'route_name', 'src_city_name', 'src_state_code',\n",
    "                            'dest_city_name', 'dest_state_code', 'route_type', 'dep_time_hr',\n",
    "                             'dep_time_min', 'travel_time_hr', 'travel_time_min']\n",
    "        \n",
    "        self.df = self.read_df()\n",
    "        \n",
    "        self.user_time = 0\n",
    "        self.current_id = ''\n",
    "        \n",
    "    def read_df(self):\n",
    "        return self._read_df(file_type='routes', names=self.column_names)\n",
    "    \n",
    "    def verify_df(self):\n",
    "        self._verify_attributes()\n",
    "    \n",
    "    def _verify_time_less_than(self, hr, min_, max_minutes):\n",
    "        hr, min_ = int(hr), int(min_)\n",
    "        \n",
    "        if (hr*60) + min_ > max_minutes:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def _check_duplicates_by(self, column_name):\n",
    "        # adapted from: https://github.com/dask/dask/issues/2952\n",
    "        self.df = self.df.reset_index().drop_duplicates([column_name]).set_index(\"index\")\n",
    "    \n",
    "    def _verify_attributes(self):\n",
    "        '''Verifies basic attributes in the table. \n",
    "        \n",
    "        We verify that each of the attributes follows the datastructures we are placing on them,\n",
    "        and we may want to add functionality to drop any rows that are duplicates.  Note: we \n",
    "        must check if the dataframe is empty so that we avoid KeyErrors.\n",
    "        \n",
    "        Note:  All of this culd be applied better but leaving for now for clarity.  Another thing\n",
    "        that we need to do here is remove duplicates of someone with the same ID.  Could just do \n",
    "        a groupby and remove all indices after the first occurance.  Looking into better ways to \n",
    "        handle the out of memory operations like the link I referenced at the beginning of the \n",
    "        '''\n",
    "#         https://docs.dask.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.dropna\n",
    "#             drop na for most attributes, but some are fine to be na\n",
    "        # drop nan rows for columns that must contain values\n",
    "        # causes issues for masking down the line...so I made my own helper function\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # route_ID must be 5 characters and alphanumeric\n",
    "        # this could probably be improved, but I am still learning dask \n",
    "        self.df = self.df[self.df['route_id'].apply(\n",
    "            lambda x: self._verify_id(x), meta=pd.Series([], dtype='str', name='x'))]\n",
    "\n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "\n",
    "        # route_name is optional but a max of 80 characters\n",
    "        self.df = self.df[self.df['route_name'].apply(\n",
    "            lambda x: self._is_nan(x) or self._verify_str_len(x, 0, 80),\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "\n",
    "        if self._is_empty(self.df):\n",
    "                return self.df\n",
    "        \n",
    "        # src_city_name, dest_city_name are a max of 80 characters\n",
    "        self.df = self.df[self.df['src_city_name'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 80), meta=pd.Series([], dtype='str', name='x'))]\n",
    "\n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        self.df = self.df[self.df['dest_city_name'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 80), meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "            \n",
    "        # route_type is 1 character and can be the integers 0, 1, or 2\n",
    "        self.df = self.df[self.df['route_type'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 1) and x in ['0', '1', '2'],\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # dep_time_hr is up to 2 characters and is an integer from 0-23\n",
    "        self.df = self.df[self.df['dep_time_hr'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 2) and self._verify_int_value(x, 0, 23),\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # travel_time_hr is up to 2 characters and is an integer from 0-72\n",
    "        self.df = self.df[self.df['travel_time_hr'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 2) and self._verify_int_value(x, 0, 72),\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # travel_time_min is up to 2 characters and is an integer from 0-59\n",
    "        self.df = self.df[self.df['travel_time_min'].apply(\n",
    "            lambda x: self._verify_str_len(x, 1, 2) and self._verify_int_value(x, 0, 59),\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # total travel time must not exceed 72 hrs\n",
    "        self.df = self.df[self.df[['travel_time_hr', 'travel_time_min']].apply(\n",
    "            lambda x: self._verify_time_less_than(*x, 72*60),\n",
    "            meta=pd.Series([], dtype='str', name='x'), axis=1)]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        # src_state_code and dest_state_code must be 2 characters and valid state codes\n",
    "        self.df = self.df[self.df['src_state_code'].apply(\n",
    "            lambda x: self._verify_str_len(x, 2, 2) and x in self.state_codes,\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        if self._is_empty(self.df):\n",
    "            return self.df\n",
    "        \n",
    "        self.df = self.df[self.df['dest_state_code'].apply(\n",
    "            lambda x: self._verify_str_len(x, 2, 2) and x in self.state_codes,\n",
    "            meta=pd.Series([], dtype='str', name='x'))]\n",
    "        \n",
    "        # remove duplicate IDs\n",
    "        self._check_duplicates_by('route_id')\n",
    "        \n",
    "        \n",
    "    ''' All functions below should be removed at a later time if we do not \n",
    "        need them.\n",
    "    '''\n",
    "    def grouper(self, column_name):\n",
    "        a = self.df.groupby(column_name)\n",
    "#         t = a.apply(self.checker, meta=pd.Series([], dtype='str', name='x'))\n",
    "        \n",
    "        t = a.apply(lambda x: self.test(x), meta=pd.Series([], dtype='str', name='x'))\n",
    "#         t = a.applymap(lambda x: self.test(x), meta=pd.Series([], dtype='str', name='x'))\n",
    "        return t\n",
    "    \n",
    "    def test(self, x):\n",
    "        self.user_time = 0\n",
    "        \n",
    "        t = x.applymap(lambda a: self.a)\n",
    "        return t\n",
    "    \n",
    "    def a(self, t):\n",
    "        print(t)\n",
    "    \n",
    "    def checker(self, x):\n",
    "        user_time = 0\n",
    "#         print(f'user time: {user_time} x: {x}')\n",
    "#         print(f'user time: {user_time} x: {x}\\n\\n')\n",
    "        return x.applymap(lambda row: self.row_check)\n",
    "#         user_time +=1\n",
    "        \n",
    "        \n",
    "    def row_check(self, row):\n",
    "        print(row)\n",
    "        return 0\n",
    "        \n",
    "    \n",
    "test_file = './test_csvs/routes/edited_Lin_Routes.csv'\n",
    "dr = RouteReader(test_file)\n",
    "# dr.df.head()\n",
    "dr.verify_df()\n",
    "dr.df.head()\n",
    "# print(dr.df.head())\n",
    "# dr.grouper('route_id').head()\n",
    "\n",
    "# v = dr._check_duplicates_by('route_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a  b\n",
       "0  a  0\n",
       "1  b  1\n",
       "2  a  0\n",
       "3  a  2\n",
       "4  b  5"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'a': ['a', 'b', 'a', 'a', 'b'],\n",
    "    'b': [0, 1, 0, 2, 5],\n",
    "    })\n",
    "ddf = dd.from_pandas(df, 2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>int64</td>\n",
       "      <td>int64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "<div>Dask Name: aggregate-agg, 5 tasks</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                   b      c\n",
       "npartitions=1              \n",
       "               int64  int64\n",
       "                 ...    ...\n",
       "Dask Name: aggregate-agg, 5 tasks"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['c'] = [1, 2, 1, 1, 2]\n",
    "ddf = dd.from_pandas(df, 2)\n",
    "nunique = dd.Aggregation(\n",
    "    name=\"nunique\",\n",
    "    chunk=lambda s: s.apply(lambda x: list(set(x))),\n",
    "    agg=lambda s0: s0.obj.groupby(level=list(range(s0.obj.index.nlevels))).sum(),\n",
    "    finalize=lambda s1: s1.apply(lambda final: len(set(final))),\n",
    "    )\n",
    "ddf.groupby('a').agg({'b':nunique, 'c':nunique})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible time overlap solution?\n",
    "\n",
    "https://stackoverflow.com/questions/57876479/efficient-way-to-compute-difference-of-all-rows-in-dask-dataframe\n",
    "https://stackoverflow.com/questions/60721290/how-to-apply-a-custom-function-to-groups-in-a-dask-dataframe-using-multiple-col\n",
    "https://docs.dask.org/en/latest/array.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dask.dataframe.core.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100A</td>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100B</td>\n",
       "      <td>1</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100C</td>\n",
       "      <td>1</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100A</td>\n",
       "      <td>1</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100B</td>\n",
       "      <td>1</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>100C</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>100A</td>\n",
       "      <td>1</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>200A</td>\n",
       "      <td>2</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>100A</td>\n",
       "      <td>2</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>100B</td>\n",
       "      <td>2</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>100C</td>\n",
       "      <td>2</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>100A</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>100B</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>100C</td>\n",
       "      <td>2</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>100C</td>\n",
       "      <td>A13</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A399</td>\n",
       "      <td>A13</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100A</td>\n",
       "      <td>A13</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100B</td>\n",
       "      <td>A13</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100C</td>\n",
       "      <td>A13</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100A</td>\n",
       "      <td>A13</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>100B</td>\n",
       "      <td>A13</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>100A</td>\n",
       "      <td>4B7</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>B933</td>\n",
       "      <td>4B7</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>B933</td>\n",
       "      <td>4B7</td>\n",
       "      <td>W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>100D</td>\n",
       "      <td>4B7</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>B933</td>\n",
       "      <td>4B7</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1  2\n",
       "0   100A    1  M\n",
       "1   100B    1  T\n",
       "2   100C    1  W\n",
       "3   100A    1  U\n",
       "4   100B    1  F\n",
       "5   100C    1  S\n",
       "6   100A    1  s\n",
       "7   200A    2  M\n",
       "8   100A    2  T\n",
       "9   100B    2  W\n",
       "10  100C    2  U\n",
       "11  100A    2  F\n",
       "12  100B    2  S\n",
       "13  100C    2  s\n",
       "14  100C  A13  M\n",
       "15  A399  A13  T\n",
       "16  100A  A13  W\n",
       "17  100B  A13  U\n",
       "18  100C  A13  F\n",
       "19  100A  A13  S\n",
       "20  100B  A13  s\n",
       "21  100A  4B7  M\n",
       "22  B933  4B7  T\n",
       "23  B933  4B7  W\n",
       "24  100D  4B7  U\n",
       "25  B933  4B7  F"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "import dask\n",
    "\n",
    "assignment_csv_path = './test_csvs/assignments/Lin_Assignment.csv'\n",
    "\n",
    "df = dask.dataframe.read_csv(assignment_csv_path, dtype='str', header=None)\n",
    "# df = dd.read_csv(assignment_csv_path, dtype='str')\n",
    "print(type(df))\n",
    "df.head(26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
